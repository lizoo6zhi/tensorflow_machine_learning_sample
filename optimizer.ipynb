{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">目前最流行的5中优化器：Momentum(动量优化)、NAG(Nesterov梯度加速)、AdaGrad、RMSProp、Adam，所有的优化算法都是对梯度下降算法进行不断的优化，对原始梯度下降算法增加惯性和环境感知因素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "momentum优化的一个简单思想：考虑物体运动惯性，想象一个保龄球在光滑表面滚下一个平缓的坡度，最开始会很慢，但是会迅速地恢复动力，直到达到最终速度（假设又一定的摩擦力核空气阻力）\n",
    "\n",
    "momentum优化关注以前的梯度是多少，公式：\n",
    "\n",
    "$(1)m \\leftarrow \\beta m + \\eta \\nabla _\\theta J(\\theta)$\n",
    "\n",
    "$(2)\\theta \\leftarrow \\theta - m$\n",
    "\n",
    "超参数$\\beta$称为动量，其必须设置在0(高摩擦)和1(零摩擦)之间，默认值为0.9\n",
    "\n",
    "可以很容易地验证当梯度保持一个常量，最终速度(即权重的最大值)就等于梯度乘以学习率乘以$\\frac{1}{1-\\beta}$,当$\\beta = 0.9$时，那么最终速度等于10倍梯度乘以学习率，所有momentum优化最终会比梯度下降快10倍，在不适用批量归一化的深度神经网络中，高层最终常会产生不同尺寸的输入，因此使用momentum优化会很有帮助，同时还会帮助跨过局部最优\n",
    "\n",
    "由于又动量，优化器可能会超调一点，然后返回，再超调，来回震荡多次后，最后稳定在最小值，这也是系统中要有一些摩擦的原因之一，它可以帮助摆脱震荡，从而加速收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nesterov梯度加速"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式：\n",
    "\n",
    "$(1)m \\leftarrow \\beta m + \\eta \\nabla _\\theta J(\\theta + \\beta m)$\n",
    "\n",
    "$(2)\\theta \\leftarrow \\theta - m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与momentum唯一不同的是用$\\theta + \\beta m$来测量梯度，这个小调整有效是因为在通常情况下，动量矢量会指向正确的方向，所以在该方向相对远的地方使用梯度会比在原有地方更准确一些"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9，use_nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad对于简单的二次问题一般表现都不错，但是在训练神经网络时却经常很早就停滞了，学习速率缩小得很多，在到达全局最优前算法就停止了，所以尽管tensorflow又AdagradOptimizer，也不要用它来训练深度神经网络\n",
    "公式：\n",
    "\n",
    "$(1)s \\leftarrow s + \\nabla _\\theta J(\\theta) \\otimes \\nabla _\\theta J(\\theta)$\n",
    "\n",
    "$(2)\\theta \\leftarrow \\theta - \\eta \\nabla _\\theta J(\\theta) \\oslash \\sqrt{s+\\varepsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad降速太快而且没有办法收敛到全局最优，RMSProp算法却通过仅积累最近迭代中得梯度(而非从训练开始得梯度)解决这个问题，它通在第一步使用指数衰减开实现\n",
    "公式：\n",
    "\n",
    "$(1)s \\leftarrow \\beta s + (1-\\beta)\\nabla _\\theta J(\\theta) \\otimes \\nabla _\\theta J(\\theta)$\n",
    "\n",
    "$(2)\\theta \\leftarrow \\theta - \\eta \\nabla _\\theta J(\\theta) \\oslash \\sqrt{s+\\varepsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "衰减率$\\eta$通常为0.9\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,momentum=0.9,decay=0.9,epsilon=0.9)\n",
    "\n",
    "除去非常简单得问题，这个优化器得表现几乎全部优于AdaGrad，同时表现也基本都优于Momentum优化和NAG，事实上在Adam优化出现之前，它是众多研究者所推荐得优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam代表了自适应力矩估计，集合了Momentum优化和RmsProp的想法，类似Momentum优化，它会跟踪过去梯度的指数衰减平均值，同时也类似RMSProp,它会跟踪过去梯度平方的指数衰减平均值，\n",
    "\n",
    "Adam算法:\n",
    "\n",
    "$(1)m \\leftarrow \\beta_1 m + (1-\\beta_i) \\nabla _\\theta J(\\theta)$\n",
    "\n",
    "$(2)s \\leftarrow \\beta_2s +(1-\\beta_2)\\nabla _\\theta J(\\theta) \\otimes \\nabla _\\theta J(\\theta)$\n",
    "\n",
    "$(3)m \\leftarrow \\frac{m}{1-\\beta_1^T}$\n",
    "\n",
    "$(4)s \\leftarrow \\frac{s}{1-\\beta_2^T}$\n",
    "\n",
    "$(5)\\theta \\leftarrow \\theta - \\eta m\\oslash \\sqrt{s+\\varepsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：T表示迭代次数（从1开始）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Adam优化器对mnist进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "epoch:0,accuracy:0.945900022983551\n",
      "epoch:1,accuracy:0.9574999809265137\n",
      "epoch:2,accuracy:0.9635000228881836\n",
      "epoch:3,accuracy:0.9693999886512756\n",
      "epoch:4,accuracy:0.970300018787384\n",
      "epoch:5,accuracy:0.9704999923706055\n",
      "epoch:6,accuracy:0.9758999943733215\n",
      "epoch:7,accuracy:0.9757999777793884\n",
      "epoch:8,accuracy:0.9768999814987183\n",
      "epoch:9,accuracy:0.9783999919891357\n",
      "epoch:10,accuracy:0.9783999919891357\n",
      "epoch:11,accuracy:0.9642999768257141\n",
      "epoch:12,accuracy:0.9779999852180481\n",
      "epoch:13,accuracy:0.9799000024795532\n",
      "epoch:14,accuracy:0.9760000109672546\n",
      "epoch:15,accuracy:0.977400004863739\n",
      "epoch:16,accuracy:0.9819999933242798\n",
      "epoch:17,accuracy:0.9781000018119812\n",
      "epoch:18,accuracy:0.9661999940872192\n",
      "epoch:19,accuracy:0.9779000282287598\n",
      "stop\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected,batch_norm\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "n_input = 784\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_output = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_input),name='X')\n",
    "Y = tf.placeholder(tf.int64,shape=(None,10),name='Y')\n",
    "#归一化参数\n",
    "is_training = tf.placeholder(tf.bool,shape=(),name='is_training')\n",
    "bn_params = {'is_training':is_training,'decay':0.99,'updates_collections':None}\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    with tf.contrib.framework.arg_scope([fully_connected],normalizer_fn=batch_norm,normalizer_params=bn_params):\n",
    "        hidden1 = fully_connected(X,n_hidden1,activation_fn=tf.nn.elu,scope='hidden1')\n",
    "        hidden2 = fully_connected(hidden1,n_hidden2,activation_fn=tf.nn.elu,scope='hidden2')\n",
    "        y_prab = fully_connected(hidden2,n_output,activation_fn=tf.nn.softmax,scope='output')\n",
    "with tf.name_scope('train'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=y_prab))\n",
    "    learning_rate = tf.placeholder(tf.float32,shape=(),name='learning_rate')\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "with tf.name_scope('accuracy'):\n",
    "    prab_bool = tf.equal(tf.argmax(y_prab,1),tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(prab_bool,tf.float32))\n",
    "with tf.name_scope('tensorboard_mnist'):\n",
    "    file_writer = tf.summary.FileWriter('./tensorboard/',tf.get_default_graph())\n",
    "    accuracy_summary = tf.summary.scalar('accuracy',accuracy)\n",
    "with tf.name_scope('saver'):\n",
    "    saver = tf.train.Saver()\n",
    "with tf.name_scope('collection'):\n",
    "    tf.add_to_collection('logits',y_prab)\n",
    "    \n",
    "epoches = 20\n",
    "batch_size = 100\n",
    "n_batches = mnist.train.num_examples // batch_size\n",
    "rate = 0.1\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epoches):\n",
    "        for batch in range(n_batches):\n",
    "            x_batch,y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer,feed_dict={X:x_batch,Y:y_batch,learning_rate:rate,is_training:True})\n",
    "        result = sess.run([accuracy,accuracy_summary],feed_dict={X:mnist.test.images,Y:mnist.test.labels,\n",
    "                                                                 learning_rate:rate,is_training:False})\n",
    "        \n",
    "        file_writer.add_summary(result[1],epoch)\n",
    "        print('epoch:{},accuracy:{}'.format(epoch,result[0]))\n",
    "    saver.save(sess,'./model/model_final.ckpt',global_step=5)\n",
    "    print('stop')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

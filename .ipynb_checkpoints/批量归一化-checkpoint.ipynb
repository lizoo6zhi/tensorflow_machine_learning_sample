{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">为了解决在深度神经网络训练初期降低梯度消失/爆炸问题，Sergey loffe和Christian Szegedy提出了使用批量归一化的技术的方案，该技术包括在每一层激活函数之前在模型里加一个操作，简单零中心化和归一化输入，之后再通过每层的两个新参数(一个缩放，另一个移动)缩放和移动结果，话句话说，这个操作让模型学会最佳模型和每层输入的平均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量归一化原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)$\\mu_B = \\frac{1}{m_B}\\sum_{i=1}^{m_B}x^{(i)}$ #经验平均值，评估整个小批量B\n",
    "\n",
    "(2)$\\theta_B = \\frac{1}{m_B}\\sum_{i=1}^{m_b}(x^{(i)} - \\mu_B)^2$ #评估整个小批量B的方差\n",
    "\n",
    "(3)$x_{(i)}^* = \\frac{x^{(i)} - \\mu_B}{\\sqrt{\\theta_B^2+\\xi}}$#零中心化和归一化\n",
    "\n",
    "(4)$z^{(i)} = \\lambda x_{(i)}^* + \\beta$#将输入进行缩放和移动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在测试期间，没有小批量的数据来计算经验平均值和标准方差，所有可以简单地用整个训练集的平均值和标准方差来代替，在训练过程中可以用变动平均值有效计算出来\n",
    "\n",
    "但是，批量归一化的确也给模型增加了一些复杂度和运行代价，使得神经网络的预测速度变慢，所以如果逆需要快速预测，可能需要在进行批量归一化之前先检查以下ELU+He初始化的表现如何"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.layers.batch_normalization使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 函数原型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def batch_normalization(inputs,\n",
    "                        axis=-1,\n",
    "                        momentum=0.99,\n",
    "                        epsilon=1e-3,\n",
    "                        center=True,\n",
    "                        scale=True,\n",
    "                        beta_initializer=init_ops.zeros_initializer(),\n",
    "                        gamma_initializer=init_ops.ones_initializer(),\n",
    "                        moving_mean_initializer=init_ops.zeros_initializer(),\n",
    "                        moving_variance_initializer=init_ops.ones_initializer(),\n",
    "                        beta_regularizer=None,\n",
    "                        gamma_regularizer=None,\n",
    "                        beta_constraint=None,\n",
    "                        gamma_constraint=None,\n",
    "                        training=False,\n",
    "                        trainable=True,\n",
    "                        name=None,\n",
    "                        reuse=None,\n",
    "                        renorm=False,\n",
    "                        renorm_clipping=None,\n",
    "                        renorm_momentum=0.99,\n",
    "                        fused=None,\n",
    "                        virtual_batch_size=None,\n",
    "                        adjustment=None):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用注意事项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)使用batch_normalization需要三步：\n",
    "\n",
    "    a.在卷积层将激活函数设置为None\n",
    "    b.使用batch_normalization\n",
    "    c.使用激活函数激活\n",
    "    \n",
    "    例子：\n",
    "    inputs = tf.layers.dense(inputs,self.n_neurons,\n",
    "                                       kernel_initializer=self.initializer,\n",
    "                                       name = 'hidden%d'%(layer+1))\n",
    "    if self.batch_normal_momentum:\n",
    "        inputs = tf.layers.batch_normalization(inputs,momentum=self.batch_normal_momentum,train=self._training)\n",
    "\n",
    "    inputs = self.activation(inputs,name = 'hidden%d_out'%(layer+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)在训练时，将参数training设置为True，在测试时，将training设置为False,同时要特别注意update_ops的使用\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    需要在每次训练时更新，可以使用sess.run(update_ops)\n",
    "    也可以：\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用mnist数据集进行简单测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加一层隐藏层步不使用batch_normalization和使用batch_normalization进行对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "x_train,y_train = mnist.train.images,mnist.train.labels\n",
    "x_test,y_test = mnist.test.images,mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "def dnn(inputs,n_hiddens=1,n_neurons=100,initializer=he_init,activation=tf.nn.elu,batch_normalization=None,training=None):\n",
    "    for layer in range(n_hiddens):\n",
    "        inputs = tf.layers.dense(inputs,n_neurons,kernel_initializer=initializer,name = 'hidden%d'%(layer+1))\n",
    "        if batch_normalization is not None:   \n",
    "            inputs = tf.layers.batch_normalization(inputs,momentum=batch_normalization,training=training)\n",
    "        inputs = activation(inputs,name = 'hidden%d'%(layer+1))\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,loss:1.5318093299865723,accuracy:0.9289000034332275\n",
      "epoch:1,loss:1.5274109840393066,accuracy:0.9332000017166138\n",
      "epoch:2,loss:1.5136224031448364,accuracy:0.9476000070571899\n",
      "epoch:3,loss:1.5105195045471191,accuracy:0.9509000182151794\n",
      "epoch:4,loss:1.5040708780288696,accuracy:0.957099974155426\n",
      "epoch:5,loss:1.5074492692947388,accuracy:0.9537000060081482\n",
      "epoch:6,loss:1.5091605186462402,accuracy:0.9516000151634216\n",
      "epoch:7,loss:1.5038185119628906,accuracy:0.957099974155426\n",
      "epoch:8,loss:1.5015618801116943,accuracy:0.9595999717712402\n",
      "epoch:9,loss:1.5023324489593506,accuracy:0.9588000178337097\n",
      "epoch:10,loss:1.5084644556045532,accuracy:0.9524999856948853\n",
      "epoch:11,loss:1.5018837451934814,accuracy:0.9592000246047974\n",
      "epoch:12,loss:1.5075123310089111,accuracy:0.9535999894142151\n",
      "epoch:13,loss:1.5019420385360718,accuracy:0.9585999846458435\n",
      "epoch:14,loss:1.5048881769180298,accuracy:0.9559000134468079\n",
      "epoch:15,loss:1.502847671508789,accuracy:0.9580000042915344\n",
      "epoch:16,loss:1.4985114336013794,accuracy:0.9624999761581421\n",
      "epoch:17,loss:1.5025790929794312,accuracy:0.9585000276565552\n",
      "epoch:18,loss:1.4982507228851318,accuracy:0.9628999829292297\n",
      "epoch:19,loss:1.5000163316726685,accuracy:0.9610000252723694\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "n_inputs = 28*28\n",
    "n_hidden = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "Y = tf.placeholder(tf.int32,shape=(None,n_outputs),name='Y')\n",
    "\n",
    "training = tf.placeholder_with_default(False,shape=(),name='tarining')\n",
    "dnn_outputs = dnn(X)\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs,n_outputs,kernel_initializer = he_init,name='logits')\n",
    "y_proba = tf.nn.softmax(logits,name='y_proba')\n",
    "xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=y_proba)\n",
    "loss = tf.reduce_mean(xentropy,name='loss')\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "correct = tf.equal(tf.argmax(Y,1),tf.argmax(y_proba,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "\n",
    "epoches = 20\n",
    "batch_size = 100\n",
    "np.random.seed(42)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "rnd_index = np.random.permutation(len(x_train))\n",
    "n_batches = len(x_train) // batch_size\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epoches):       \n",
    "        for batch_index in np.array_split(rnd_index,n_batches):\n",
    "            x_batch,y_batch = x_train[batch_index],y_train[batch_index]\n",
    "            feed_dict = {X:x_batch,Y:y_batch,training:True}\n",
    "            sess.run(train_op,feed_dict=feed_dict)\n",
    "        loss_val,accuracy_val = sess.run([loss,accuracy],feed_dict={X:x_test,Y:y_test,training:False})\n",
    "        print('epoch:{},loss:{},accuracy:{}'.format(epoch,loss_val,accuracy_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成随机数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- 从正态分布中输出随机数\n",
    "    \n",
    "    def random_normal(shape,\n",
    "                  mean=0.0,\n",
    "                  stddev=1.0,\n",
    "                  dtype=dtypes.float32,\n",
    "                  seed=None,\n",
    "                  name=None):\n",
    "\n",
    "-- 从均匀分布中输出随机数\n",
    "\n",
    "    def random_uniform(shape,\n",
    "                   minval=0,\n",
    "                   maxval=None,\n",
    "                   dtype=dtypes.float32,\n",
    "                   seed=None,\n",
    "                   name=None):\n",
    "\n",
    "-- 截断的正态分布函数，生成的值遵循一个正态分布，但不会大于平均值2个标准差\n",
    "\n",
    "    def truncated_normal(shape,\n",
    "                     mean=0.0,\n",
    "                     stddev=1.0,\n",
    "                     dtype=dtypes.float32,\n",
    "                     seed=None,\n",
    "                     name=None):\n",
    "-- 沿着要被洗牌的张量的第一个维度，随机打乱\n",
    "\n",
    "    def random_shuffle(value, seed=None, name=None):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全链接函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def fully_connected(inputs,\n",
    "                    num_outputs,\n",
    "                    activation_fn=nn.relu,\n",
    "                    normalizer_fn=None,\n",
    "                    normalizer_params=None,\n",
    "                    weights_initializer=initializers.xavier_initializer(),\n",
    "                    weights_regularizer=None,\n",
    "                    biases_initializer=init_ops.zeros_initializer(),\n",
    "                    biases_regularizer=None,\n",
    "                    reuse=None,\n",
    "                    variables_collections=None,\n",
    "                    outputs_collections=None,\n",
    "                    trainable=True,\n",
    "                    scope=None):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- 批量归一化\n",
    "\n",
    "    在默认情况下，batch_norm()函数提供了所有的参数，只中心化、归一化和对输入进行偏移，但是并不缩放\n",
    "    \n",
    "    def batch_norm(inputs,\n",
    "               decay=0.999,\n",
    "               center=True,\n",
    "               scale=False,\n",
    "               epsilon=0.001,\n",
    "               activation_fn=None,\n",
    "               param_initializers=None,\n",
    "               param_regularizers=None,\n",
    "               updates_collections=ops.GraphKeys.UPDATE_OPS,\n",
    "               is_training=True,\n",
    "               reuse=None,\n",
    "               variables_collections=None,\n",
    "               outputs_collections=None,\n",
    "               trainable=True,\n",
    "               batch_weights=None,\n",
    "               fused=None,\n",
    "               data_format=DATA_FORMAT_NHWC,\n",
    "               zero_debias_moving_mean=False,\n",
    "               scope=None,\n",
    "               renorm=False,\n",
    "               renorm_clipping=None,\n",
    "               renorm_decay=0.99,\n",
    "               adjustment=None):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- 构造参数范围\n",
    "\n",
    "    def arg_scope(list_ops_or_scope, **kwargs):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 示例版本1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "epoch:0,accuracy:0.6664000153541565\n",
      "epoch:1,accuracy:0.6736999750137329\n",
      "epoch:2,accuracy:0.7346000075340271\n",
      "epoch:3,accuracy:0.7444000244140625\n",
      "epoch:4,accuracy:0.7468000054359436\n",
      "epoch:5,accuracy:0.7490000128746033\n",
      "epoch:6,accuracy:0.7506999969482422\n",
      "epoch:7,accuracy:0.75\n",
      "epoch:8,accuracy:0.8264999985694885\n",
      "epoch:9,accuracy:0.8309999704360962\n",
      "epoch:10,accuracy:0.8341000080108643\n",
      "epoch:11,accuracy:0.8356999754905701\n",
      "epoch:12,accuracy:0.8352000117301941\n",
      "epoch:13,accuracy:0.8371999859809875\n",
      "epoch:14,accuracy:0.8373000025749207\n",
      "epoch:15,accuracy:0.8374999761581421\n",
      "epoch:16,accuracy:0.8410000205039978\n",
      "epoch:17,accuracy:0.8421000242233276\n",
      "epoch:18,accuracy:0.8411999940872192\n",
      "epoch:19,accuracy:0.8414000272750854\n",
      "stop\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "n_input = 784\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_output = 10\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    X = tf.placeholder(tf.float32,shape=(None,n_input),name='X')\n",
    "    Y = tf.placeholder(tf.int64,shape=(None,10),name='Y')\n",
    "    \n",
    "    stddev = 1/np.sqrt(n_input+n_hidden1)\n",
    "    w_1 = tf.Variable(tf.random.truncated_normal(shape=(n_input,n_hidden1),mean=0.0,stddev=stddev,seed=42),name='w_1')\n",
    "    b_1 = tf.Variable(tf.ones((n_hidden1)),name='b_1')\n",
    "    output_1 = tf.nn.elu(tf.matmul(X,w_1)+b_1)\n",
    "with tf.name_scope('hidden1'):\n",
    "    stddev = 1/np.sqrt(n_hidden1)\n",
    "    w_2 = tf.Variable(tf.random.truncated_normal(shape=(n_hidden1,n_hidden2),mean=0.0,stddev=stddev,seed=42),name='w_2')\n",
    "    b_2 = tf.Variable(tf.ones(n_hidden2),name='b_2')\n",
    "    output_2 = tf.nn.elu(tf.matmul(output_1,w_2)+b_2)\n",
    "with tf.name_scope('hidden2'):\n",
    "    stddev = 1/np.sqrt(n_hidden2)\n",
    "    w_3 = tf.Variable(tf.random.truncated_normal(shape=(n_hidden2,n_output),mean=0.0,stddev=stddev,seed=42),name='w_3')\n",
    "    b_3 = tf.Variable(tf.ones(n_output),name='b_3')\n",
    "    y_prab = tf.nn.softmax(tf.matmul(output_2,w_3)+b_3)\n",
    "with tf.name_scope('train'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=y_prab))\n",
    "    learning_rate = tf.placeholder(tf.float32,shape=(),name='learning_rate')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "with tf.name_scope('accuracy'):\n",
    "    prab_bool = tf.equal(tf.argmax(y_prab,1),tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(prab_bool,tf.float32))\n",
    "    \n",
    "epoches = 20\n",
    "batch_size = 100\n",
    "n_batches = mnist.train.num_examples // batch_size\n",
    "rate = 0.1\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epoches):\n",
    "        for batch in range(n_batches):\n",
    "            x_batch,y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer,feed_dict={X:x_batch,Y:y_batch,learning_rate:rate})\n",
    "        acc = sess.run(accuracy,feed_dict={X:mnist.test.images,Y:mnist.test.labels,learning_rate:rate})\n",
    "        print('epoch:{},accuracy:{}'.format(epoch,acc))\n",
    "    print('stop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 示例版本2:使用fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "epoch:0,accuracy:0.758400022983551\n",
      "epoch:1,accuracy:0.8450000286102295\n",
      "epoch:2,accuracy:0.8500999808311462\n",
      "epoch:3,accuracy:0.8622999787330627\n",
      "epoch:4,accuracy:0.9193999767303467\n",
      "epoch:5,accuracy:0.926800012588501\n",
      "epoch:6,accuracy:0.930899977684021\n",
      "epoch:7,accuracy:0.9323999881744385\n",
      "epoch:8,accuracy:0.9334999918937683\n",
      "epoch:9,accuracy:0.9358000159263611\n",
      "epoch:10,accuracy:0.9365000128746033\n",
      "epoch:11,accuracy:0.9380999803543091\n",
      "epoch:12,accuracy:0.9427000284194946\n",
      "epoch:13,accuracy:0.9433000087738037\n",
      "epoch:14,accuracy:0.9438999891281128\n",
      "epoch:15,accuracy:0.9455999732017517\n",
      "epoch:16,accuracy:0.9474999904632568\n",
      "epoch:17,accuracy:0.9491999745368958\n",
      "epoch:18,accuracy:0.949400007724762\n",
      "epoch:19,accuracy:0.9509000182151794\n",
      "stop\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "n_input = 784\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_output = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_input),name='X')\n",
    "Y = tf.placeholder(tf.int64,shape=(None,10),name='Y')\n",
    "    \n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = fully_connected(X,n_hidden1,activation_fn=tf.nn.elu,scope='hidden1')\n",
    "    hidden2 = fully_connected(hidden1,n_hidden2,activation_fn=tf.nn.elu,scope='hidden2')\n",
    "    y_prab = fully_connected(hidden2,n_output,activation_fn=tf.nn.softmax,scope='output')\n",
    "with tf.name_scope('train'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=y_prab))\n",
    "    learning_rate = tf.placeholder(tf.float32,shape=(),name='learning_rate')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "with tf.name_scope('accuracy'):\n",
    "    prab_bool = tf.equal(tf.argmax(y_prab,1),tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(prab_bool,tf.float32))\n",
    "    \n",
    "epoches = 20\n",
    "batch_size = 100\n",
    "n_batches = mnist.train.num_examples // batch_size\n",
    "rate = 0.1\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epoches):\n",
    "        for batch in range(n_batches):\n",
    "            x_batch,y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer,feed_dict={X:x_batch,Y:y_batch,learning_rate:rate})\n",
    "        acc = sess.run(accuracy,feed_dict={X:mnist.test.images,Y:mnist.test.labels,learning_rate:rate})\n",
    "        print('epoch:{},accuracy:{}'.format(epoch,acc))\n",
    "    print('stop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 示例版本3：添加批量标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "epoch:0,accuracy:0.9165999889373779\n",
      "epoch:1,accuracy:0.9365000128746033\n",
      "epoch:2,accuracy:0.9470999836921692\n",
      "epoch:3,accuracy:0.949999988079071\n",
      "epoch:4,accuracy:0.9550999999046326\n",
      "epoch:5,accuracy:0.9574999809265137\n",
      "epoch:6,accuracy:0.9593999981880188\n",
      "epoch:7,accuracy:0.9606999754905701\n",
      "epoch:8,accuracy:0.9611999988555908\n",
      "epoch:9,accuracy:0.963100016117096\n",
      "epoch:10,accuracy:0.9635999798774719\n",
      "epoch:11,accuracy:0.9642999768257141\n",
      "epoch:12,accuracy:0.9653000235557556\n",
      "epoch:13,accuracy:0.9656999707221985\n",
      "epoch:14,accuracy:0.9674000144004822\n",
      "epoch:15,accuracy:0.9672999978065491\n",
      "epoch:16,accuracy:0.9681000113487244\n",
      "epoch:17,accuracy:0.9695000052452087\n",
      "epoch:18,accuracy:0.9695000052452087\n",
      "epoch:19,accuracy:0.9690999984741211\n",
      "stop\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected,batch_norm\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "n_input = 784\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_output = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_input),name='X')\n",
    "Y = tf.placeholder(tf.int64,shape=(None,10),name='Y')\n",
    "#归一化参数\n",
    "is_training = tf.placeholder(tf.bool,shape=(),name='is_training')\n",
    "bn_params = {'is_training':is_training,'decay':0.99,'updates_collections':None}\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    with tf.contrib.framework.arg_scope([fully_connected],normalizer_fn=batch_norm,normalizer_params=bn_params):\n",
    "        hidden1 = fully_connected(X,n_hidden1,activation_fn=tf.nn.elu,scope='hidden1')\n",
    "        hidden2 = fully_connected(hidden1,n_hidden2,activation_fn=tf.nn.elu,scope='hidden2')\n",
    "        y_prab = fully_connected(hidden2,n_output,activation_fn=tf.nn.softmax,scope='output')\n",
    "with tf.name_scope('train'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=y_prab))\n",
    "    learning_rate = tf.placeholder(tf.float32,shape=(),name='learning_rate')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "with tf.name_scope('accuracy'):\n",
    "    prab_bool = tf.equal(tf.argmax(y_prab,1),tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(prab_bool,tf.float32))\n",
    "with tf.name_scope('tensorboard_mnist'):\n",
    "    file_writer = tf.summary.FileWriter('./tensorboard/',tf.get_default_graph())\n",
    "    accuracy_summary = tf.summary.scalar('accuracy',accuracy)\n",
    "with tf.name_scope('saver'):\n",
    "    saver = tf.train.Saver()\n",
    "with tf.name_scope('collection'):\n",
    "    tf.add_to_collection('logits',y_prab)\n",
    "    \n",
    "epoches = 20\n",
    "batch_size = 100\n",
    "n_batches = mnist.train.num_examples // batch_size\n",
    "rate = 0.1\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epoches):\n",
    "        for batch in range(n_batches):\n",
    "            x_batch,y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer,feed_dict={X:x_batch,Y:y_batch,learning_rate:rate,is_training:True})\n",
    "        result = sess.run([accuracy,accuracy_summary],feed_dict={X:mnist.test.images,Y:mnist.test.labels,\n",
    "                                                                 learning_rate:rate,is_training:False})\n",
    "        \n",
    "        file_writer.add_summary(result[1],epoch)\n",
    "        print('epoch:{},accuracy:{}'.format(epoch,result[0]))\n",
    "    saver.save(sess,'./model/model_final.ckpt',global_step=5)\n",
    "    print('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model_final.ckpt-5\n",
      "Tensor(\"dnn/output/Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "[[0.03515256 0.03342073 0.03411456 0.02732576 0.0330676  0.0203246\n",
      "  0.02946526 0.06812413 0.03293247 0.68607235]\n",
      " [0.02412354 0.03439866 0.02281778 0.02638844 0.02411861 0.03686505\n",
      "  0.03394691 0.0331142  0.73169863 0.03252817]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('./model/model_final.ckpt-5.meta')\n",
    "    saver.restore(sess,'./model/model_final.ckpt-5')\n",
    "    y_prab = tf.get_collection('logits')[0]\n",
    "    print(y_prab)\n",
    "    y_prabval = y_prab.eval(feed_dict={X:mnist.test.images[6000:6002],Y:mnist.test.labels[6000:6002],\n",
    "                                         learning_rate:0.1,is_training:False})\n",
    "    \n",
    "    print(y_prabval)\n",
    "    print(mnist.test.labels[6000:6002])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
